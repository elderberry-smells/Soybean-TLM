{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soybean Trait Linked Marker Tool\n",
    "\n",
    "This tool will analyze the trait linked marker panels currently in the HTMA soybean molecular lab by reading a kraken export file.  The tool utilizes python 3.5.0.\n",
    "\n",
    "version: 1.0\n",
    "\n",
    "## Haplotype panels\n",
    "there are currently 5 marker panels in the soybean molecular lab that are trait linked.  The following dictionary outlines the different panels and which combination of alleles for each assay will produce a 'Trait' call.  The panels themselves are made up of various markers that vary in length from a panel of 2 assays to a panel of 3.  \n",
    "\n",
    "Some panels will only have 1 combination of alleles that will produce a trait, but others can have 2 combinations which are dictated in the list of dictionaries below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haplotype_panel = {\n",
    "               'rps1_haplotypes' : [\n",
    "                   # the following allele combinations will produce 'Trait'\n",
    "                   {'DAS_Gm03_4458273_G_A Zygosity Call': 'G:G',\n",
    "                    'DAS_Gm03_3838302_A_T Zygosity Call': 'A:A'},\n",
    "                \n",
    "                   {'DAS_Gm03_4458273_G_A Zygosity Call': 'A:A',\n",
    "                    'DAS_Gm03_3838302_A_T Zygosity Call': 'T:T'}\n",
    "                    ],\n",
    "\n",
    "                'rps3a_haplotypes' : [\n",
    "                    # the following allele combinations will produce 'Trait'\n",
    "                    {'Gm13_28913825_G_A Zygosity Call': 'G:G',\n",
    "                     'DAS_Gm13_29375136_A_G Zygosity Call': 'G:G'},\n",
    "                     ],\n",
    "\n",
    "                'rhg1_haplotypes' : [\n",
    "                   # the following allele combinations will produce 'Trait'\n",
    "                   {'DAS_Gm18_1607524_A_C Zygosity Call': 'C:C',\n",
    "                    'DAS_Gm18_1634714_A_G Zygosity Call': 'G:G',\n",
    "                    'DAS_Gm18_1686081_A_G Zygosity Call': 'G:G'}\n",
    "                    ],\n",
    "\n",
    "                'scc_3_haplotype' : [\n",
    "                   # the following allele combinations will produce 'Trait'\n",
    "                   {'Gm03_46333142_G_A Zygosity Call': 'G:G',\n",
    "                    'Gm03_46508111_A_C Zygosity Call': 'A:A'}\n",
    "                   ],\n",
    "                 \n",
    "                 'scc_11_haplotype' : [\n",
    "                    # the following allele combinations will produce 'Trait'\n",
    "                    {'Gm11_37179389_G_T Zygosity Call': 'G:G',\n",
    "                     'NCSB_002654 Zygosity Call': 'A:A'}\n",
    "                    ]\n",
    "    }\n",
    "\n",
    "# for future use - look to see if panel is an unknown and return unknown\n",
    "unknown_haplotypes =  {  # the following combinations will produce 'Unknown'\n",
    "               'rps1_haplotypes' :[{'placeholder':'allele'}],\n",
    "\n",
    "                'rps3a_haplotypes' : [{'placeholder':'allele'}],\n",
    "\n",
    "                'rhg1_haplotypes' : [{'placeholder':'allele'}],\n",
    "\n",
    "                'scc_3_haplotype' : [{'placeholder':'allele'}],\n",
    "                 \n",
    "                 'scc_11_haplotype' : [{'placeholder':'allele'}]\n",
    "                }\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a Kraken File output\n",
    "\n",
    "Kraken exports as an xlsx file, so we have the scipt read through a specific folder and find any xlsx files that need analysis.  It converts the files to a csv using pandas, and then makes a list of csv files for running through the TLM tool analysis for a report output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "path = r'C:\\Users\\U590135\\.spyder-py3\\projects\\Soybean TLM'\n",
    "xlextension = 'xlsx'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "xlsx_result = [xl for xl in glob.glob('*.{}'.format(xlextension))]\n",
    "\n",
    "# convert the xlsx files into csv files\n",
    "xl_files = []\n",
    "for xlsx_files in xlsx_result:\n",
    "    if '_report' in xlsx_files:\n",
    "        continue\n",
    "    else:\n",
    "        xl_files.append(xlsx_files)\n",
    "\n",
    "for i in xl_files:\n",
    "    df_temp = pd.read_excel(i, index=False, encoding='utf-8')\n",
    "    get_name = i.find('.')\n",
    "    csv_name = i[:get_name] + '.csv'\n",
    "    df_temp.to_csv(csv_name, index=False, encoding='utf-8')\n",
    "    os.remove(i)  # delete the original XLSX file so there are no duplicates\n",
    "\n",
    "#  get a list of all the csv files to run the script on\n",
    "csv_result = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "completed_path = r'C:\\Users\\U590135\\.spyder-py3\\projects\\Soybean TLM\\completed'\n",
    "\n",
    "results_files = []\n",
    "for i in csv_result:\n",
    "    if 'temp_calls' in i:\n",
    "        continue\n",
    "    else:\n",
    "        results_files.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the files through the TLM tool\n",
    "\n",
    "The file is read for the fieldnames as a header list and the possible panel summary headers are appended to that list for a temporary file creation later.  Each file that is in the folder for analysis will go through the following steps in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fhand in results_files:  # for each csv in the folder, run the analysis\n",
    "    print('Analyzing ' + fhand + '...')\n",
    "\n",
    "    with open(fhand, newline='') as kraken:\n",
    "        reader = csv.DictReader(kraken)\n",
    "        kraken_headers = reader.fieldnames\n",
    "        \n",
    "        panel_summaries = ['Rps1 Zygosity Call', 'Rps3 Zygosity Call', 'Rhg1 Summary',\n",
    "                           'SCC_3 Summary', 'SCC_11 Summary']  \n",
    "    \n",
    "        panel_list = []  # empty list where we can store which panels were run - for final merge sequence\n",
    "        header_list = kraken_headers\n",
    "\n",
    "        for summs in panel_summaries:\n",
    "            header_list.append(summs)  # add the possible panel summaries to the headers\n",
    "        \n",
    "        with open('temp_calls.csv', 'w', newline='') as resultfile:\n",
    "            writer = csv.DictWriter(resultfile, fieldnames=header_list)\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the kraken headers for the assay panels\n",
    "\n",
    "Run the kraken_header list through the indexAssays function to find the location of **every TLM panel present** in the kraken file.  The function will return the column location (index) where the panel is present for all assays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: try and index rps1_haplotype panel from header lists and return a list of column locations\n",
    "kraken_headers = ['Box', 'Well', 'Project', 'Sample Tube Number', 'Loc Seq#',\n",
    "                  'DAS_Gm03_4458273_G_A Zygosity Call', 'DAS_Gm03_3838302_A_T Zygosity Call']\n",
    "panel_list =[]\n",
    "\n",
    "rps1_index = indexAssays('rps1_haplotypes', kraken_headers)\n",
    "rps1_len = int(len(rps1_index))\n",
    "if rps1_len > 1:\n",
    "    panel_list.append('rps1_haplotypes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexAssays(panel, headers):\n",
    "    \n",
    "    global haplotype_panel\n",
    "    \n",
    "    panel_index = []\n",
    "    index_assay = haplotype_panel[panel][0]\n",
    "\n",
    "    for key in index_assay:\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            if headers.index(key) in panel_index:  # if panel has been appended already, skip\n",
    "                continue\n",
    "            else:\n",
    "                panel_index.append(headers.index(key))  # get index of panel (if exists)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return panel_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rps1_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing the control wells and converting the 'Project' well to utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in reader:\n",
    "                remove_controls = ['A2', 'A02', 'A3', 'A03', 'A4', 'A04', 'A5', 'A05', 'A6', 'A06']\n",
    "                if row['Well'] in remove_controls:  # remove controls from report\n",
    "                    continue\n",
    "                else:    \n",
    "                    row['Project'] = row['Project'].encode('utf-8')  # encode the Project column as utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The getCall function\n",
    "this function dictates the trait linked call of the haplotypes in the panels present in the panel.  There are a few call varieties that can be a result:  Trait, Wildtype, Seg, No call, No data, and blank.\n",
    "\n",
    "The data that gets passed into the getCall function is a dictionary that is created by isolating the panels into a dictionary (from the previous indexing step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCall(dic, panel):\n",
    "    '''determine the samples call for each panel in kraken file'''\n",
    "    global haplotype_panel\n",
    "    global unknown_haplotypes\n",
    "     \n",
    "    seg = 0\n",
    "    dupe = 0\n",
    "    no_data = 0\n",
    "    empty = 0\n",
    "    \n",
    "    #determine if the combination of assays is the Trait haplotype\n",
    "    for hap in haplotype_panel[panel]:\n",
    "        if dic == hap:\n",
    "            return 'Trait'\n",
    "                 \n",
    "    # for future use: determine if the combination of assays is the unknown haplotype\n",
    "    for hap in unknown_haplotypes[panel]:\n",
    "        if dic == hap:\n",
    "            return 'Unknown'                         \n",
    "                \n",
    "    # determine if there is a seg/no call or no data in the panel\n",
    "    for assay, allele in dic.items():\n",
    "        if allele == 'No Data':\n",
    "            no_data += 1\n",
    "        elif allele == 'Empty':\n",
    "            empty += 1\n",
    "        elif allele == 'Unused':\n",
    "            no_data += 1\n",
    "        elif allele == 'Dupe':\n",
    "            dupe += 1\n",
    "        elif allele[0] != allele[-1]: \n",
    "            seg += 1\n",
    "             \n",
    "    # using the counts of each type, return the call        \n",
    "    if empty > 0:\n",
    "        return ''\n",
    "    elif no_data > 0:\n",
    "        return 'No Data'\n",
    "    elif dupe > 0:\n",
    "        return 'No Call'\n",
    "    elif seg > 0:\n",
    "        return 'Seg'\n",
    "    else:\n",
    "        return 'Wildtype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example of different haplotypes passing into the getCall function\n",
    "\n",
    "example1 = {'DAS_Gm03_4458273_G_A Zygosity Call': 'G:G', 'DAS_Gm03_3838302_A_T Zygosity Call': 'A:A'}\n",
    "example2 = {'DAS_Gm03_4458273_G_A Zygosity Call': 'A:A', 'DAS_Gm03_3838302_A_T Zygosity Call': 'T:T'}\n",
    "example3 = {'DAS_Gm03_4458273_G_A Zygosity Call': 'A:A', 'DAS_Gm03_3838302_A_T Zygosity Call': 'A:A'}\n",
    "example4 = {'DAS_Gm03_4458273_G_A Zygosity Call': 'G:A', 'DAS_Gm03_3838302_A_T Zygosity Call': 'A:A'}\n",
    "example5 = {'DAS_Gm03_4458273_G_A Zygosity Call': 'G:G', 'DAS_Gm03_3838302_A_T Zygosity Call': 'Dupe'}\n",
    "\n",
    "example1_call = getCall(example1, 'rps1_haplotypes')\n",
    "example2_call = getCall(example2, 'rps1_haplotypes')\n",
    "example3_call = getCall(example3, 'rps1_haplotypes')\n",
    "example4_call = getCall(example4, 'rps1_haplotypes')\n",
    "example5_call = getCall(example5, 'rps1_haplotypes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Trait', 'Trait', 'Wildtype', 'Seg', 'No Call')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1_call, example2_call, example3_call, example4_call, example5_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various panels will pass similar to the example above but use the data from the csv being read to populate the dictionary being passed into the getCall function.  They are all referencing the specific panel key in the haplotype_panel to determine if the Trait haplotype is present.  If not, it determines if there is any no data/seg/dupe and anything left is a wildtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if rps1_len > 1:\n",
    "    ''' RPS1 is present so make a dictionary out of the panel, pass into getCall to make a summary column'''\n",
    "    rps1_index.sort()  # make sure column names are in order\n",
    "    dict_rps1 = {}\n",
    "\n",
    "    for i in rps1_index:\n",
    "        dict_rps1[kraken_headers[i]] = row[kraken_headers[i]]\n",
    "\n",
    "    rps1_call = getCall(dict_rps1, 'rps1_haplotypes')\n",
    "    row['Rps1 Zygosity Call'] = rps1_call\n",
    "\n",
    "if rps3_len > 1:\n",
    "    ''' RPS3A is present so make a dictionary out of the panel, pass into getCall to make a summary column'''\n",
    "    rps3_index.sort()  # make sure column names are in order\n",
    "    dict_rps3 = {}\n",
    "\n",
    "    for i in rps3_index:\n",
    "        dict_rps3[kraken_headers[i]] = row[kraken_headers[i]]\n",
    "\n",
    "    rps3_call = getCall(dict_rps3, 'rps3a_haplotypes')\n",
    "    row['Rps3 Zygosity Call'] = rps3_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the data together\n",
    "The final report will be made using pandas dataframes of each individual haplotype present.  This is to ensure the summary column follows the panel of assays rather than at the end of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tlm_df = pd.read_csv('temp_calls.csv', encoding='UTF-8')\n",
    "\n",
    "# get rid of the b' prefix in project column\n",
    "tlm_df['Project'] = tlm_df['Project'].map(lambda x: x.lstrip(\"b'\").rstrip(\"'\"))\n",
    "\n",
    " # reformat the file and write the TLM summary report to excel\n",
    "krak_info = ['Box', 'Well', 'Project', 'Sample Tube Number', 'Loc Seq#', 'RowId']\n",
    "\n",
    "df_initial = tlm_df[krak_info]  # sample and well info dataframe only (no molecular data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # merge initial with rps1 dataframe if panel is in the kraken file\n",
    "    \n",
    "    if 'rps1_haplotypes' in panel_list:\n",
    "        rps1_headers = krak_info\n",
    "        for i in rps1_index:\n",
    "            rps1_headers.append(kraken_headers[i])\n",
    "        rps1_headers.append('Rps1 Zygosity Call')\n",
    "        rps1_df = tlm_df[rps1_headers]\n",
    "        merge1 = pd.merge(left=df_initial, right=rps1_df, how='left')\n",
    "    else:\n",
    "        tlm_df.drop(['Rps1 Zygosity Call'], axis=1, inplace=True, errors='ignore')\n",
    "        merge1 = df_initial\n",
    "    \n",
    "    # merge that with rps3 dataframe if panel is in the kraken file\n",
    "    \n",
    "    if 'rps3a_haplotypes' in panel_list:\n",
    "        rps3_headers = krak_info\n",
    "        for i in rps3_index:\n",
    "            rps3_headers.append(kraken_headers[i])\n",
    "        rps3_headers.append('Rps3 Zygosity Call')\n",
    "        rps3_df = tlm_df[rps3_headers]\n",
    "        merge2 = pd.merge(left=merge1, right=rps3_df, how='left')\n",
    "    else:\n",
    "        tlm_df.drop(['Rps3 Zygosity Call'], axis=1, inplace=True, errors='ignore')\n",
    "        merge2 = merge1\n",
    "\n",
    "# repeat process for each panel in sequence until you have gone through all panels (adding 1 to merge number each time).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final merge sequence and writing of new report \n",
    "the merging is completed by merging the data that was not a part of the TLM panels (regular zygosity data).  Once that is in a dataframe, write the file as xlsx to a \"completed\" folder, and remove any copies of the file from the initial folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge with final dataframe and write the file (gets any assays not in panel tacked on to end of kraken report)\n",
    "    \n",
    "    merge_final = pd.merge(left=merge5, right=tlm_df, how='left')\n",
    "    \n",
    "    find_the_dot = fhand.find('.')\n",
    "    final_name = fhand[:find_the_dot] + '.xlsx'\n",
    "    results_writer = ExcelWriter(final_name)\n",
    "    \n",
    "    os.chdir(completed_path)\n",
    "    merge_final.to_excel(results_writer, 'Report', index=False)\n",
    "    results_writer.save()\n",
    "    \n",
    "    os.chdir(path) \n",
    "    os.remove('temp_calls.csv') # delete the temporary call file\n",
    "    os.remove(fhand)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
